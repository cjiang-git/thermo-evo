{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d1544-df05-470e-85e0-36d3d34cf5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_model, save_model, load_file\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig,AutoModel,TrainingArguments, Trainer,DataCollatorWithPadding, EarlyStoppingCallback, get_constant_schedule_with_warmup\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from stripedhyena.tokenizer import CharLevelTokenizer\n",
    "from stripedhyena.model import StripedHyena\n",
    "from stripedhyena.layers import VocabParallelEmbedding\n",
    "from stripedhyena.utils import dotdict\n",
    "import yaml\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7d113-2498-4e33-98d4-c1ba373c7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = './processed_data'\n",
    "cpu_cnt = multiprocessing.cpu_count()\n",
    "max_length = 300\n",
    "model_name = './evo-1-8k-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c057b7a-d4da-462c-9c1b-164394ca0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "\n",
    "\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a4a8c-d7f5-45e4-966f-846b1c3faf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,padding='max_length',bos_token='\\x00')\n",
    "tokenizer.pad_token = '\\x01'\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,max_length=max_length,padding='max_length')\n",
    "#Evo utilizes a custom tokenizer named \"CharLevelTokenizer\", huggingface version does not have pad_token defined. Thus pad_token is manually defined to have same value as the pad_token utilizes in CharLevelTokenizer\n",
    "\n",
    "def prefix_function(d):\n",
    "    d['dna_seq'] = tokenizer.bos_token + d['dna_seq']\n",
    "    return d\n",
    "\n",
    "def tokenize_function(d):\n",
    "    return tokenizer(d['dna_seq'], padding='max_length',max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec451f-30b6-4038-8cba-89fd3fca8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dsets(data_path,batch_size):\n",
    "    dsets = load_from_disk(data_path)\n",
    "    #dsets = dsets.map(prefix_function, num_proc=cpu_cnt)\n",
    "    dsets = dsets.map(tokenize_function, batched=True, num_proc=cpu_cnt)\n",
    "    useful_col = ['input_ids', 'attention_mask', 'labels']\n",
    "    dsets = dsets.remove_columns([col for col in dsets['train'].column_names if col not in useful_col])\n",
    "    train_loader = DataLoader(\n",
    "        dsets['train'],\n",
    "        collate_fn=data_collator,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size\n",
    "        )\n",
    "    val_loader = DataLoader(\n",
    "            dsets['val'],\n",
    "            collate_fn=data_collator,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    test_loader = DataLoader(\n",
    "            dsets['test'],\n",
    "            collate_fn=data_collator,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    return train_loader,val_loader,test_loader, len(dsets['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead07e37-0e53-40fb-8ad6-78879f338502",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(model_name,trust_remote_code=True)\n",
    "model_config.use_cache = False\n",
    "model_og = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=model_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "config_path = './custom_config.yaml'\n",
    "#state_dict = model_og.backbone.state_dict()\n",
    "config = yaml.safe_load(open(config_path, 'rb').read())\n",
    "global_config = dotdict(config, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40bcbcc-2594-4a10-b5c3-aec32f617da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyena(StripedHyena):\n",
    "    def __init_(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding_layer = VocabParallelEmbedding(config)\n",
    "        self.norm = RMSNorm(config) if config.get(\"final_norm\", True) else None\n",
    "        #self.unembed = self.embedding_layer if config.tie_embeddings else VocabParallelEmbedding(config)\n",
    "\n",
    "        if config.get(\"use_flashfft\", \"True\"):\n",
    "            try:\n",
    "                from flashfftconv import FlashFFTConv\n",
    "\n",
    "                self.flash_fft = FlashFFTConv(config.seqlen, dtype=torch.bfloat16)\n",
    "            except ImportError:\n",
    "                \"flashfftconv not installed\"\n",
    "        else:\n",
    "            self.flash_fft = None\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            get_block(config, layer_idx, flash_fft=self.flash_fft) for layer_idx in range(config.num_layers)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, inference_params_dict=None, padding_mask=None):\n",
    "        L = x.shape[1]\n",
    "        x = self.embedding_layer.embed(x)\n",
    "        if inference_params_dict is not None:\n",
    "            x, inference_params_dict_out = self.stateful_forward(\n",
    "                x,\n",
    "                inference_params_dict=inference_params_dict,\n",
    "            )\n",
    "        else:\n",
    "            x, inference_params_dict_out = self.stateless_forward(x, padding_mask=padding_mask)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        #Removed unembedding\n",
    "        #x = self.unembed.unembed(x)\n",
    "        return x, inference_params_dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8dc79-d160-41db-9c6e-c7f131c10e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_hyena = MyHyena(global_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d635db3-8c7b-4da3-814a-5cc870aa7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoForRegression(nn.Module):\n",
    "  def __init__(self): \n",
    "    super(EvoForRegression,self).__init__() \n",
    "\n",
    "    self.model = custom_hyena\n",
    "    self.dropout = nn.Dropout(0.1) \n",
    "    #First_Token representation\n",
    "\n",
    "    self.lin = nn.Linear(model_config.hidden_size,model_config.hidden_size) \n",
    "    self.out = nn.Linear(model_config.hidden_size,1) \n",
    "      \n",
    "\n",
    "  def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "    #Extract outputs from the hyena\n",
    "    outputs = self.model(x=input_ids, padding_mask=attention_mask)[0][:,0,:]\n",
    "    #Add custom layers\n",
    "    outputs = self.lin(outputs)    \n",
    "    outputs = nn.ReLU()(outputs)  \n",
    "    out = self.out(outputs)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914526f8-86ff-4c20-a388-940ffb086555",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,val_loader,test_loader, test_steps = prepare_dsets(data_files,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6e36fb-388e-470c-9a09-5bd0effed9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model,OUT_DIR,batch_size,warmup_steps = 10,checkpointing_steps=1000):\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    \n",
    "    train_loader,val_loader,test_loader, epoch_size = prepare_dsets(data_files,batch_size)\n",
    "    \n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=1,\n",
    "        mixed_precision=\"bf16\"\n",
    "    )\n",
    "    \n",
    "    accelerator.print(f\"Total GPUS: {accelerator.num_processes}\")\n",
    "    \n",
    "    \n",
    "    num_training_steps = epoch_size // batch_size\n",
    "    num_epoch_steps = num_training_steps // 1\n",
    "\n",
    "                                         \n",
    "    model,test_loader= accelerator.prepare(model, test_loader)\n",
    "    \n",
    "\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    completed_steps = 0\n",
    "    \n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "    \n",
    "    log_loss = os.path.join(OUT_DIR,'test_log')\n",
    "    loss_file = open(log_loss, \"w\", encoding=\"utf-8\") if accelerator.is_main_process else None\n",
    "    #loss_file = open(log_loss, \"a\" if resume_from_checkpoint else \"w\", encoding=\"utf-8\") if accelerator.is_main_process else None\n",
    "    model.eval()\n",
    "    pred_outputs = []\n",
    "    for step, batch in enumerate(test_loader):\n",
    "            loss_log = None\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "                gathered = (outputs.squeeze(),batch['labels'].squeeze())\n",
    "                loss = loss_fn(outputs.squeeze(),batch['labels'].squeeze())\n",
    "                pred_outputs.append(gathered)\n",
    "                if accelerator.sync_gradients:\n",
    "                    loss_log = {\n",
    "                        \"loss\": loss.item(),\n",
    "                        \"epoch\": completed_steps / num_epoch_steps\n",
    "                    }\n",
    "                    accelerator.log(loss_log, step=completed_steps)\n",
    "                    if loss_file is not None:\n",
    "                        loss_file.write(f\"{loss_log['loss']},\")\n",
    "                        loss_file.flush()\n",
    "    \n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                if loss_log is not None:\n",
    "                    progress_bar.set_postfix(loss_log)\n",
    "                completed_steps += 1\n",
    "                \n",
    "    \n",
    "                if completed_steps > 0:\n",
    "                    if completed_steps % checkpointing_steps == 0:\n",
    "                        output_dir = f\"step_{completed_steps}\"\n",
    "                        if OUT_DIR is not None:\n",
    "                            output_dir = os.path.join(\n",
    "                                OUT_DIR, output_dir)\n",
    "                        accelerator.save_state(output_dir)\n",
    "\n",
    "        \n",
    "            if completed_steps >= num_training_steps:\n",
    "                print(completed_steps)\n",
    "                print(num_training_steps)\n",
    "                print('broke')\n",
    "                break\n",
    "    return pred_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841eb7ac-3a88-413f-9c52-7208bdf27baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = EvoForRegression()\n",
    "path_to_tst = './loop/64/final.pickle'\n",
    "model.load_state_dict(torch.load(path_to_tst))\n",
    "OUT_DIR =  './loop/64/test_outs'\n",
    "test_out = eval_model(model,OUT_DIR,128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd36ce-0f69-4571-bcbd-d6241543bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "output_vals = []\n",
    "target_vals = []\n",
    "for output_val, target_val  in test_out:\n",
    "    output_val = output_val.cpu()\n",
    "    target_val = target_val.cpu()\n",
    "    output_vals.extend(output_val)\n",
    "    target_vals.extend(target_val)\n",
    "mean_squared_error(target_vals,output_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827b7d0-4ed7-4a16-842b-d0cc5ec521a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "training_logs = pd.read_csv('./loop/64/train_log',sep=',', header=None)\n",
    "plt.plot(training_logs.values[0])\n",
    "plt.title(\"Training Loss for model 1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c3b90-e717-4712-af7b-f8f5e1075cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(training_logs.values[0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3892ecc-0ef5-41fb-bd71-57cf60387a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
