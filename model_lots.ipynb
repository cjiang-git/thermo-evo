{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed12b8-dd24-4573-883a-a4e35f6ae7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig,AutoModel,TrainingArguments, Trainer,DataCollatorWithPadding, EarlyStoppingCallback, get_constant_schedule_with_warmup\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from stripedhyena.tokenizer import CharLevelTokenizer\n",
    "from stripedhyena.model import StripedHyena\n",
    "from stripedhyena.layers import VocabParallelEmbedding\n",
    "from stripedhyena.utils import dotdict\n",
    "import yaml\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from torch.utils.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f02902-79c7-4466-bd45-4c586513d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = './processed_data'\n",
    "cpu_cnt = multiprocessing.cpu_count()\n",
    "max_length = 300\n",
    "model_name = './evo-1-8k-base'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3164c7-93a9-4773-b28b-4057aa25931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "\n",
    "\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa93b0-9da6-4914-ba91-bedb1b58fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,padding='max_length',bos_token='\\x00')\n",
    "tokenizer.pad_token = '\\x01'\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,max_length=max_length,padding='max_length')\n",
    "#Evo utilizes a custom tokenizer named \"CharLevelTokenizer\", huggingface version does not have pad_token defined. Thus pad_token is manually defined to have same value as the pad_token utilizes in CharLevelTokenizer\n",
    "\n",
    "def prefix_function(d):\n",
    "    d['dna_seq'] = tokenizer.bos_token + d['dna_seq']\n",
    "    return d\n",
    "\n",
    "def tokenize_function(d):\n",
    "    return tokenizer(d['dna_seq'], padding='max_length',max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26474337-bdcd-4e98-90c7-7f46a46e35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dsets(data_path,batch_size):\n",
    "    dsets = load_from_disk(data_path)\n",
    "    dsets = dsets.map(prefix_function, num_proc=cpu_cnt)\n",
    "    dsets = dsets.map(tokenize_function, batched=True, num_proc=cpu_cnt)\n",
    "    useful_col = ['input_ids', 'attention_mask', 'labels']\n",
    "    dsets = dsets.remove_columns([col for col in dsets['train'].column_names if col not in useful_col])\n",
    "    train_loader = DataLoader(\n",
    "        dsets['train'],\n",
    "        collate_fn=data_collator,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size\n",
    "        )\n",
    "    val_loader = DataLoader(\n",
    "            dsets['val'],\n",
    "            collate_fn=data_collator,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    test_loader = DataLoader(\n",
    "            dsets['test'],\n",
    "            collate_fn=data_collator,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    return train_loader,val_loader,test_loader, len(dsets['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d40196-89fc-499d-926c-03827a33828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,OUT_DIR,data_path,batch_size = 128,warmup_steps = 10,epochs = 3,learning_rate=2e-5,checkpointing_steps=1000):\n",
    "\n",
    "    train_loader,val_loader,test_loader, epoch_size = prepare_dsets(data_path,batch_size)\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=1,\n",
    "        mixed_precision=\"bf16\"\n",
    "    )\n",
    "    \n",
    "    accelerator.print(f\"Total GPUS: {accelerator.num_processes}\")\n",
    "    \n",
    "    \n",
    "    num_training_steps = epoch_size * epochs // batch_size\n",
    "    num_epoch_steps = num_training_steps // epochs\n",
    "\n",
    "    optim = torch.optim.AdamW(list(model.parameters()), lr=learning_rate)\n",
    "    scheduler = transformers.get_cosine_schedule_with_warmup(optim, num_warmup_steps=warmup_steps,num_training_steps = num_training_steps)\n",
    "                                                  \n",
    "    model,optim, train_loader, scheduler= accelerator.prepare(model,optim, train_loader, scheduler)\n",
    "    \n",
    "    accelerator.register_for_checkpointing(scheduler)\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    completed_steps = 0\n",
    "    \n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    \n",
    "\n",
    "    stp_lst = []\n",
    "    for i in os.listdir(OUT_DIR):\n",
    "        if i[:4]=='step':\n",
    "            stp_lst.append(int(i[5:]))\n",
    "\n",
    "    if stp_lst != []:\n",
    "        resume_from_checkpoint = True\n",
    "        resume_step = max(stp_lst)\n",
    "        latest = 'step_' + str(resume_step)\n",
    "        checkpoint_dir = os.path.join(OUT_DIR,latest)\n",
    "        path = os.path.basename(checkpoint_dir)\n",
    "        accelerator.print(\n",
    "                f\"Resuming from checkpoint {latest}\")\n",
    "        accelerator.load_state(checkpoint_dir, strict=False)\n",
    "    else: \n",
    "        resume_from_checkpoint = False\n",
    "    \n",
    "    if resume_from_checkpoint and resume_step is not None:\n",
    "        train_loader_skipped = accelerator.skip_first_batches(\n",
    "            train_loader, resume_step % num_epoch_steps)\n",
    "        completed_steps += resume_step\n",
    "        progress_bar.update(resume_step)\n",
    "        accelerator.print(f\"Resuming training from step {resume_step}\")\n",
    "        epoch = resume_step // num_epoch_steps\n",
    "        skip = True\n",
    "    else:\n",
    "        epoch = 0\n",
    "        skip = False\n",
    "\n",
    "    \n",
    "    log_loss = os.path.join(OUT_DIR,'train_log')\n",
    "    #loss_file = open(log_loss, \"w\", encoding=\"utf-8\") if accelerator.is_main_process else None\n",
    "    loss_file = open(log_loss, \"a\" if resume_from_checkpoint else \"w\", encoding=\"utf-8\") if accelerator.is_main_process else None\n",
    "    \n",
    "    print(f'Starting epoch: {epoch}, Ending Epoch: {epochs}, Total training steps: {num_training_steps}')\n",
    "    \n",
    "    for epoch in range(epoch,epochs):\n",
    "        model.train()\n",
    "        if skip:\n",
    "            for step, batch in enumerate(train_loader_skipped):\n",
    "                loss_log = None\n",
    "                skip = False\n",
    "                with accelerator.accumulate(model):\n",
    "                    outputs = model(**batch)\n",
    "                    loss = loss_fn(outputs.squeeze(),batch['labels'].squeeze())\n",
    "                    accelerator.backward(loss)\n",
    "                    \n",
    "                    if accelerator.sync_gradients:\n",
    "                        last_lr = scheduler.get_last_lr()\n",
    "                        if isinstance(last_lr, list):\n",
    "                            last_lr = last_lr[0]\n",
    "                        loss_log = {\n",
    "                            \"loss\": loss.item(),\n",
    "                            \"epoch\": completed_steps / num_epoch_steps,\n",
    "                            \"learning_rate\": last_lr\n",
    "                        }\n",
    "                        accelerator.log(loss_log, step=completed_steps)\n",
    "                        if loss_file is not None:\n",
    "                            loss_file.write(f\"{loss_log['loss']},\")\n",
    "                            loss_file.flush()\n",
    "        \n",
    "                        \n",
    "                    optim.step()\n",
    "                    scheduler.step()\n",
    "                    optim.zero_grad()\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    progress_bar.update(1)\n",
    "                    if loss_log is not None:\n",
    "                        progress_bar.set_postfix(loss_log)\n",
    "                    completed_steps += 1\n",
    "                    \n",
    "        \n",
    "                    if completed_steps > 0:\n",
    "                        if completed_steps % checkpointing_steps == 0:\n",
    "                            output_dir = f\"step_{completed_steps}\"\n",
    "                            if OUT_DIR is not None:\n",
    "                                output_dir = os.path.join(\n",
    "                                    OUT_DIR, output_dir)\n",
    "                            accelerator.save_state(output_dir)\n",
    "    \n",
    "            \n",
    "                if completed_steps >= num_training_steps:\n",
    "                    print(completed_steps)\n",
    "                    print(num_training_steps)\n",
    "                    print('broke')\n",
    "                    break\n",
    "        else: \n",
    "            for step, batch in enumerate(train_loader):\n",
    "                loss_log = None\n",
    "                with accelerator.accumulate(model):\n",
    "                    outputs = model(**batch)\n",
    "                    loss = loss_fn(outputs.squeeze(),batch['labels'].squeeze())\n",
    "                    accelerator.backward(loss)\n",
    "                    \n",
    "                    if accelerator.sync_gradients:\n",
    "                        last_lr = scheduler.get_last_lr()\n",
    "                        if isinstance(last_lr, list):\n",
    "                            last_lr = last_lr[0]\n",
    "                        loss_log = {\n",
    "                            \"loss\": loss.item(),\n",
    "                            \"epoch\": completed_steps / num_epoch_steps,\n",
    "                            \"learning_rate\": last_lr\n",
    "                        }\n",
    "                        accelerator.log(loss_log, step=completed_steps)\n",
    "                        if loss_file is not None:\n",
    "                            loss_file.write(f\"{loss_log['loss']},\")\n",
    "                            loss_file.flush()\n",
    "        \n",
    "                        \n",
    "                    optim.step()\n",
    "                    scheduler.step()\n",
    "                    optim.zero_grad()\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    progress_bar.update(1)\n",
    "                    if loss_log is not None:\n",
    "                        progress_bar.set_postfix(loss_log)\n",
    "                    completed_steps += 1\n",
    "                    \n",
    "        \n",
    "                    if completed_steps > 0:\n",
    "                        if completed_steps % checkpointing_steps == 0:\n",
    "                            output_dir = f\"step_{completed_steps}\"\n",
    "                            if OUT_DIR is not None:\n",
    "                                output_dir = os.path.join(\n",
    "                                    OUT_DIR, output_dir)\n",
    "                            accelerator.save_state(output_dir)\n",
    "    \n",
    "            \n",
    "                if completed_steps >= num_training_steps:\n",
    "                    print(completed_steps)\n",
    "                    print(num_training_steps)\n",
    "                    print('broke')\n",
    "                    break\n",
    "                    \n",
    "        \n",
    "            \n",
    "        if completed_steps >= num_training_steps:\n",
    "            print(completed_steps)\n",
    "            print(num_training_steps)\n",
    "            print('broke')\n",
    "            break\n",
    "    \n",
    "    accelerator.print(\"Training Finished\")\n",
    "    accelerator.end_training()\n",
    "\n",
    "    if OUT_DIR is not None:\n",
    "            accelerator.print(f\"Saving model to {OUT_DIR}\")\n",
    "    \n",
    "            accelerator.wait_for_everyone()\n",
    "    \n",
    "            if accelerator.distributed_type == DistributedType.FSDP:\n",
    "                full_state_dict_config = FullStateDictConfig(\n",
    "                    offload_to_cpu=True, rank0_only=True)\n",
    "                with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, full_state_dict_config):\n",
    "                    state_dict = accelerator.get_state_dict(model, unwrap=False)\n",
    "            else:\n",
    "                state_dict = accelerator.get_state_dict(model)\n",
    "    \n",
    "            torch.save(state_dict,os.path.join(OUT_DIR,\"final.pickle\"))\n",
    "            accelerator.print(\"Saving Finished\")\n",
    "    model, optim, train_loader, scheduler = accelerator.free_memory(model,optim, train_loader, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4f4e6-7f32-4727-818d-366171ef817e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59fa897-8ab1-4a45-bbb5-a37578f189cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(model_name,trust_remote_code=True)\n",
    "model_config.use_cache = False\n",
    "model_og = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=model_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "config_path = './custom_config.yaml'\n",
    "state_dict = model_og.backbone.state_dict()\n",
    "config = yaml.safe_load(open(config_path, 'rb').read())\n",
    "global_config = dotdict(config, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10859e-a374-41bf-9cf9-1ee24572391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class MyEmbed(VocabParallelEmbedding):\n",
    "    def __init__(self, config):\n",
    "        vocab_size, process_group, padding_idx = (\n",
    "            config.vocab_size,\n",
    "            config.get(\"process_group\", None),\n",
    "            config.get(\"padding_idx\", None),\n",
    "        )\n",
    "        self.process_group = process_group\n",
    "        if process_group is not None:\n",
    "            world_size = torch.distributed.get_world_size(process_group)\n",
    "            if vocab_size % world_size != 0:\n",
    "                raise ValueError(f\"vocab_size ({vocab_size}) must be divisible by \" f\"world_size ({world_size})\")\n",
    "            if world_size > 1 and padding_idx is not None:\n",
    "                raise RuntimeError(\"ParallelEmbedding does not support padding_idx\")\n",
    "        else:\n",
    "            world_size = 1\n",
    "        super().__init__(\n",
    "            vocab_size // world_size,\n",
    "            embedding_dim=config.hidden_size,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "    def embed(self, input: Tensor) -> Tensor:\n",
    "        if self.process_group is None:\n",
    "            return self.forward(input)\n",
    "        else:\n",
    "            rank = torch.distributed.get_rank(self.process_group)\n",
    "            vocab_size = self.num_embeddings\n",
    "            vocab_start_index, vocab_end_index = (\n",
    "                rank * vocab_size,\n",
    "                (rank + 1) * vocab_size,\n",
    "            )\n",
    "            # Create a mask of valid vocab ids (1 means it needs to be masked).\n",
    "            input_ids_mask = (input < vocab_start_index) | (input >= vocab_end_index)\n",
    "            input = input - vocab_start_index\n",
    "            input[input_ids_mask] = 0\n",
    "            embeddings = self.forward(input)\n",
    "            embeddings[input_ids_mask] = 0.0\n",
    "            # Reduce to the global process group\n",
    "            torch.distributed.all_reduce(embeddings, group=self.process_group)\n",
    "            return embeddings\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f49a13-c505-4735-88c1-a8b0ae91176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyena(StripedHyena):\n",
    "    def __init_(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding_layer = VocabParallelEmbedding(config)\n",
    "        self.norm = RMSNorm(config) if config.get(\"final_norm\", True) else None\n",
    "        #self.unembed = self.embedding_layer if config.tie_embeddings else VocabParallelEmbedding(config)\n",
    "\n",
    "        if config.get(\"use_flashfft\", \"True\"):\n",
    "            try:\n",
    "                from flashfftconv import FlashFFTConv\n",
    "\n",
    "                self.flash_fft = FlashFFTConv(config.seqlen, dtype=torch.bfloat16)\n",
    "            except ImportError:\n",
    "                \"flashfftconv not installed\"\n",
    "        else:\n",
    "            self.flash_fft = None\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            get_block(config, layer_idx, flash_fft=self.flash_fft) for layer_idx in range(config.num_layers)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, inference_params_dict=None, padding_mask=None):\n",
    "        L = x.shape[1]\n",
    "        x = self.embedding_layer.embed(x)\n",
    "        if inference_params_dict is not None:\n",
    "            x, inference_params_dict_out = self.stateful_forward(\n",
    "                x,\n",
    "                inference_params_dict=inference_params_dict,\n",
    "            )\n",
    "        else:\n",
    "            x, inference_params_dict_out = self.stateless_forward(x, padding_mask=padding_mask)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        #Removed unembedding\n",
    "        #x = self.unembed.unembed(x)\n",
    "        return x, inference_params_dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6850a-bcc3-4665-93ff-9399c47a0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_hyena = MyHyena(global_config)\n",
    "custom_hyena.load_state_dict(state_dict,strict=True)\n",
    "\n",
    "\n",
    "for p in custom_hyena.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe9559-36d8-4ff1-bdb7-c5a2536eb639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoForRegression(nn.Module):\n",
    "  def __init__(self): \n",
    "    super(EvoForRegression,self).__init__() \n",
    "\n",
    "    self.model = custom_hyena\n",
    "    self.dropout = nn.Dropout(0.1) \n",
    "    #First_Token representation\n",
    "\n",
    "    self.lin = nn.Linear(model_config.hidden_size,model_config.hidden_size) \n",
    "    self.out = nn.Linear(model_config.hidden_size,1) \n",
    "      \n",
    "\n",
    "  def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "    #Extract outputs from the hyena\n",
    "    outputs = self.model(x=input_ids, padding_mask=attention_mask)[0][:,0,:]\n",
    "    #Add custom layers\n",
    "    outputs = self.dropout(outputs)\n",
    "\n",
    "    outputs = self.lin(outputs)\n",
    "      \n",
    "    outputs = nn.ReLU()(outputs)  # (bs, dim)\n",
    "    outputs = self.dropout(outputs)  # (bs, dim)\n",
    "    out = self.out(outputs)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b4498-d01e-4312-bb14-883a9a4aa67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EvoForRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3154f-47ae-4c84-9bed-6c698eb76c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine batchsize effect on model\n",
    "batchsizes = [64,128]\n",
    "OUT_DIR='./loop/'\n",
    "data_path = data_files\n",
    "\n",
    "val_loss_lst = []\n",
    "for i in batchsizes:\n",
    "    outdir = OUT_DIR + str(i)\n",
    "    del model\n",
    "    model = EvoForRegression()\n",
    "    train_model(model,outdir,data_path,batch_size = i,warmup_steps = 10,epochs = 1,learning_rate=2e-5,checkpointing_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177aa225-c4c2-4861-a89f-1932f002e406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef1cb6-ed40-4491-859e-163c4f2d8ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
